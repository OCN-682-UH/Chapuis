---
title: "w8a_lecture"
author: "Micaela Chapuis"
date: "2024-10-15"
output: html_document
---

## Working with Words
```{r}
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr) # has all of Jane Austen's books in it
library(stopwords)
```

## Stringr package

There are 4 basic families of functions in the {stringr} package:

1. **Manipulation**: these functions allow you to manipulate individual characters within the strings in character vectors.
2. **Whitespace tools** to add, remove, and manipulate whitespace.
3. **Locale sensitive operations** whose operations will vary from locale to locale (different languages)
4. **Pattern matching functions.** These recognize four engines of pattern description. The most common is regular expressions, but there are three other tools.

### Manipulation
Paste words together. This can be useful if say you have a two columns of treatments and you want to combine them into one (e.g., high temp, low temp and high pH, low pH).
```{r}
paste("High temp", "Low pH")
```

```{r}
paste("High temp", "Low pH", sep = "-") # specify a separator
```

```{r}
paste0("High temp", "Low pH") # paste0() will leave no space between them
```

Working with vectors: This is very useful when making labels for your plots
```{r}
shapes <- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)
```

```{r}
two_cities <- c("best", "worst")
paste("It was the", two_cities, " of times.")
```

Individual characters  
Let's say you want to know how long a string is:
```{r}
shapes # vector of shapes
```

```{r}
str_length(shapes) #how many letters are in each word?
```

Let's say you want to extract specific characters. Do you work with sequence data? This could be super useful to exact specific bases in a sequence.
```{r}
seq_data<-c("ATCCCGTC")
str_sub(seq_data, start = 2, end = 4) # extract the 2nd to 4th base pair
```

You can also modify strings
```{r}
str_sub(seq_data, start = 3, end = 3) <- "A" # add an A in the 3rd position
seq_data
```

You can also duplicate patterns in your strings. Here I am duplicating it 2 and 3 times
```{r}
str_dup(seq_data, times = c(2, 3)) # times is the number of times to duplicate
```

### Whitespace
Say you have a column and you did not copy and paste your treatments like you learned in the first week of class. You now have some words with extra white spaces and R thinks its an entirely new word. Here is how to deal with that...
```{r}
badtreatments <- c("High", " High", "High ", "Low", "Low")
```

```{r}
str_trim(badtreatments) # removes all white spaces on both sides
```

You can also just remove from one side or the other
```{r}
str_trim(badtreatments, side = "left") # removes from just left side
```

The opposite of str_trim is str_pad, to add white space to either side
```{r}
str_pad(badtreatments, 5, side = "right") # add a white space to the right side as the 5th character
# makes everything 5 characters long
```

add a character instead of white space
```{r}
str_pad(badtreatments, 5, side = "right", pad = "1") # add a 1 to the right side as the 5th character
# if something already has 5 characters they don't get the 1
```


### Locale Sensitive
Important, these will perform differently in different places in the world/with different languages. The default is English, but you can set the language setting.  

Make everything uppercase
```{r}
x<-"I love R!"
str_to_upper(x)
```

Make it lowercase
```{r}
str_to_lower(x)
```

Make it title case (capitalize first letter of each word)
```{r}
str_to_title(x)
```

### Pattern Matching
{stringr} has functions to view, detect, locate, extract, match, replace, and split strings based on specific patterns.

View a specific pattern in a vector of strings
```{r}
data <- c("AAA", "TATA", "CTAG", "GCTT")
```

```{r}
# find all strings with an A and highlights the As
str_view(data, pattern = "A")
```

Detect a specific pattern
```{r}
str_detect(data, pattern = "A")
```

```{r}
str_detect(data, pattern = "AT")
```

Locate a pattern
```{r}
str_locate(data, pattern = "AT") # when it shows it, it says which character it starts at and ends at
```

## Regex: Regular Expressions

There are several types of regular expressions:
* Metacharacters
* Sequences
* Quantifiers
* Character classes
* POSIX character classes (Portable Operating System Interface)


### Metacharacters
The simplest form of regular expressions are those that match a single character. Most characters, including all letters and digits, are regular expressions that match themselves.   

For a language like R, there are some special characters that have reserved meaning and they are referred to as ‘Metacharacters” (table in slides). The metacharacters in Extended Regular Expressions (EREs) are:  

. \ | ( ) [ { $ * + ?  

Let's say that you have the following set of strings
```{r}
vals <- c("a.b", "b.c", "c.d")
```

And you want to replace all the "." with a space. Here is how you would do it:
```{r}
#string, pattern, replace
str_replace(vals, "\\.", " ") # \\. is the way we write the period, then we say what we want to replace it with
```

Each function in {stringr} has two forms a basic form that searches for the first instance of a character and a *_all that searches for all instances. For example:  

Let's say we had multiple "." in our character vector
```{r}
vals<-c("a.b.c", "b.c.d","c.d.e")
#string, pattern, replace
str_replace(vals, "\\.", " ")
```

str_replace only replaces the first instance. Let's try str_replace_all()
```{r}
str_replace_all(vals, "\\.", " ")
```

### Sequences
Sequences, as the name suggests refers to the sequences of characters which can match. We have shorthand versions (or anchors) for commonly used sequences in R (table in the slides)  

Let's subset the vector to only keep strings with digits
```{r}
val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d") # only retains the strings that have a digit in them
```


### Characters 
A character class or character set is a list of characters enclosed by square brackets [ ]. Character sets are used to match only one of the different characters. For example, the regex character class [aA] matches any lower case letter a or any upper case letter A. (table in slides)  

If we put a ^ inside the bracket, it means "everything other than"  
[^0-9] means match anything other than a digit  

Let's count the number of lowercase vowels in each string
```{r}
str_count(val2, "[aeiou]") # 1 vowel in test for each of them
```

```{r}
# count any digit
str_count(val2, "[0-9]") # 3 numbers in the first two "test xxx" strings, no numbers in the last one
```


### Quantifiers
There's a table in the slides  

symbol	Meaning  
^	        Beginning of String  
$	        End of String  
\n	      Newline  
+	        One or More of Previous  
*	        Zero or More of Previous  
?	        Zero or One of Previous  
{5}	      Exactly 5 of Previous  
{2, 5}	  Between 2 and 5 or Previous  
{2, }	    More than 2 of Previous  

Example: Find phone numbers  
```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
```

Make a regex that finds all the strings that contain a phone number. 
We know there is a specific pattern (3 numbers, 3 numbers, 4 numbers and it can have either a "." or "-" to separate them). 
Let's also say we know that the first number cannot be a 1

```{r}
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"


# one by one:
# [2-9] - find digits between 2 and 9 as first digit
# [0-9]{2} - then find any other two digits between 0 and 9
# [- .] - then find a dash or dot (the space between dash and dot doesn't mean anything, don't need it)
# [0-9]{3} - then find any digit with 3 digits between 0 and 9
# [- .] - find a dash or dot
# [0-9]{4} - find any digit with 4 digits between 0 and 9


```

```{r}
# Which strings contain phone numbers?
str_detect(strings, phone)
```

```{r}
# subset only the strings with phone numbers
test<-str_subset(strings, phone)
test
```

Think, pair, share: Let's clean it up. Lets replace all the "." with "-" and extract only the numbers (leaving the letters behind). Remove any extra white space. You can use a sequence of pipes.

```{r}
test %>% 
  str_replace_all("\\.", "-") %>%  # replace periods with dashes
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% # remove all the things we don't want (in this case all upper case and lowercase letters OR the \\:, and replace them with nothing)
  str_trim() # trim the white space
```


## TidyText
Package for text mining and making text tidy. This is very helpful for social sciences or anyone that uses survey data. Also, really helpful for text mining abstracts to write a review paper on a topic.  
Let's analyze a books by Jane Austen.  

The function to get all of the text from all of Jane Austen's books is austen_books()
```{r}
head(austen_books()) # text column is every line of text in each book and book is book name
tail(austen_books())
```
Let's clean it up and add a column for line and chapter
```{r}
original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text,  # regex returns TRUE or FALSE, cumsum adds up the number of TRUEs
                                     regex("^chapter [\\divxlc]", # count the chapters (starts with the word chapter followed by a digit (\\d) or roman numeral(ivxlc))
                                     ignore_case = TRUE)))) %>% #ignore whether it's lower or uppercase for the regex expression
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing... its >73000 lines...
head(original_books)
```

Because we are interest in text mining, we will want to clean this so that there is only one word per row so its tidy. In tidytext each word is refered to as a token. The function to unnest the data so that its only one word per row is unnest_tokens().
```{r}
tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
# this got rid of the text column, only keeping the new word column

head(tidy_books) # there are now >725,000 rows. Don't view the entire thing!
```

OK so we now have >735,000 rows of words.... but, some of these words are kind of useless. Words that are common and don't really have important meaning (e.g. "and","by","therefore"...). These are called stopwords. We can use the function "get_stopwords()" to essentially remove these words from our dataframe. (This function is essentially just a dataframe of unnecessary words)
```{r}
#see an example of all the stopwords already in tidy text
head(get_stopwords())
```

Use what we know from joins to remove all the stopwords  

```{r}
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords

# removes everything listed in the entire jane austen dataframe that also shows up in the stopwords dataframe

## Joining with by = join_by(word)
head(cleaned_books)
```

Let's count the most common words across all her books
```{r}
cleaned_books %>%
  count(word, sort = TRUE) # sort in descending order
```

How would we modify this code to count the most popular words by book? What about by each chapter within a book?  
We would pipe to group_by(book) or group_by(book, chapter)  

### Sentiment Analysis
There are many ways that we can now analyze this tidy dataset of text. One example is we could do a sentiment analysis (how many positive and negative words) using get_sentiments(). Sometimes it's wrong (miss is marked as negative e.g. "I miss that" but it can also be Miss)  
```{r}
sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep pos or negative words
  count(word, sentiment, sort = TRUE) # count them
head(sent_word_counts)[1:3,]
```

We can now use ggplot to visualize counts of positive and negative words in the books
```{r}
sent_word_counts %>%
  filter(n > 150) %>% # take words only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it goes from largest to smallest (most used words that are positive go at top and most used negative words at bottom)
  ggplot(aes(word, n, fill = sentiment)) +
    geom_col() +
    coord_flip() +
    labs(y = "Contribution to sentiment")
```

## Make a wordcloud
Use the {wordcloud} package to make an interactive word cloud

```{r}
words <- cleaned_books %>%
            count(word) %>% # count all the words
            arrange(desc(n))%>% # sort the words in descending order
            slice(1:100) #take the top 100

wordcloud2(words, shape = 'circle', size=0.3) # make a wordcloud out of the top 100 words
```

